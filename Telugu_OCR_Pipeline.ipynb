{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Telugu OCR Dataset Builder\n",
                "Build high-quality OCR training dataset from page images + ground truth\n",
                "\n",
                "## Setup\n",
                "1. Upload `source_images/` and `ground_truth/` to Google Drive\n",
                "2. Run this notebook"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Install dependencies\n",
                "!apt-get install -y tesseract-ocr tesseract-ocr-tel\n",
                "!pip install pytesseract rapidfuzz opencv-python tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Config - UPDATE THESE PATHS\n",
                "DRIVE_ROOT = \"/content/drive/MyDrive/telugu-ocr\"  # Your Drive folder\n",
                "SOURCE_IMAGES = f\"{DRIVE_ROOT}/source_images/images\"\n",
                "GROUND_TRUTH = f\"{DRIVE_ROOT}/ground_truth\"\n",
                "OUTPUT_DIR = f\"{DRIVE_ROOT}/dataset\"\n",
                "\n",
                "MIN_LINES = 4\n",
                "MIN_MATCH_SCORE = 95"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import pytesseract\n",
                "import cv2\n",
                "import numpy as np\n",
                "import json\n",
                "import shutil\n",
                "import re\n",
                "import os\n",
                "from pathlib import Path\n",
                "from collections import defaultdict\n",
                "from rapidfuzz import fuzz\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "SOURCE_IMAGES_DIR = Path(SOURCE_IMAGES)\n",
                "GROUND_TRUTH_DIR = Path(GROUND_TRUTH)\n",
                "OUTPUT = Path(OUTPUT_DIR)\n",
                "\n",
                "def split_into_sentences(text):\n",
                "    chunks = re.split(r'[.\\nà¥¤]+', text)\n",
                "    return [s.strip() for s in chunks if s.strip() and len(s.strip()) > 5]\n",
                "\n",
                "def find_best_sentence_range(tesseract_text, sentences):\n",
                "    if not tesseract_text.strip() or not sentences:\n",
                "        return None, 0\n",
                "    clean_tess = \" \".join(tesseract_text.split())\n",
                "    tess_len = len(clean_tess)\n",
                "    best_match, best_score = None, 0\n",
                "    n = len(sentences)\n",
                "    for size in range(1, min(16, n + 1)):\n",
                "        for start in range(n - size + 1):\n",
                "            combined = \" \".join(sentences[start:start + size])\n",
                "            if len(combined) < tess_len * 0.7:\n",
                "                continue\n",
                "            score = fuzz.ratio(clean_tess, combined)\n",
                "            if score > best_score:\n",
                "                best_score, best_match = score, combined\n",
                "    return best_match, best_score\n",
                "\n",
                "def build_story_index():\n",
                "    print(\"Building story index...\")\n",
                "    page_to_story = {}\n",
                "    for json_file in tqdm(list(GROUND_TRUTH_DIR.rglob(\"*.json\"))):\n",
                "        try:\n",
                "            with open(json_file, 'r', encoding='utf-8') as f:\n",
                "                data = json.load(f)\n",
                "        except: continue\n",
                "        pdf_stem = json_file.stem\n",
                "        stories = data.get(\"stories\", [])\n",
                "        if not stories and \"story\" in data: stories = [data[\"story\"]]\n",
                "        if not stories and \"content\" in data: stories = [data]\n",
                "        for story in stories:\n",
                "            content = story.get(\"content\", \"\")\n",
                "            if not content: continue\n",
                "            start_page = story.get(\"pdf_page_start\", 1)\n",
                "            end_page = story.get(\"pdf_page_end\", start_page)\n",
                "            for page in range(start_page, end_page + 1):\n",
                "                key = (pdf_stem, page)\n",
                "                page_to_story[key] = page_to_story.get(key, \"\") + \"\\n\" + content\n",
                "    print(f\"Indexed {len(page_to_story)} page mappings\")\n",
                "    return page_to_story\n",
                "\n",
                "def parse_page_number(filename):\n",
                "    stem = Path(filename).stem\n",
                "    match = re.search(r'page[_-]?(\\d+)', stem, re.IGNORECASE)\n",
                "    if match: return int(match.group(1))\n",
                "    match = re.search(r'(\\d+)$', stem)\n",
                "    if match: return int(match.group(1))\n",
                "    return None\n",
                "\n",
                "def extract_and_align(img_path, gt_text, output_dir, unique_id):\n",
                "    results = []\n",
                "    img = cv2.imread(str(img_path))\n",
                "    if img is None: return results\n",
                "    try:\n",
                "        data = pytesseract.image_to_data(img, lang='tel', output_type=pytesseract.Output.DICT)\n",
                "    except: return results\n",
                "    paragraphs = defaultdict(list)\n",
                "    for i in range(len(data['text'])):\n",
                "        text = data['text'][i].strip()\n",
                "        conf = int(data['conf'][i])\n",
                "        if conf > 0 and text:\n",
                "            key = (data['block_num'][i], data['par_num'][i])\n",
                "            paragraphs[key].append({\n",
                "                'text': text, 'x': data['left'][i], 'y': data['top'][i],\n",
                "                'w': data['width'][i], 'h': data['height'][i], 'line_num': data['line_num'][i]\n",
                "            })\n",
                "    sentences = split_into_sentences(gt_text)\n",
                "    if not sentences: return results\n",
                "    img_h, img_w = img.shape[:2]\n",
                "    para_idx = 0\n",
                "    for key, words in paragraphs.items():\n",
                "        if not words: continue\n",
                "        x_min = min(w['x'] for w in words) - 10\n",
                "        y_min = min(w['y'] for w in words) - 10\n",
                "        x_max = max(w['x'] + w['w'] for w in words) + 10\n",
                "        y_max = max(w['y'] + w['h'] for w in words) + 10\n",
                "        x_min, y_min = max(0, x_min), max(0, y_min)\n",
                "        x_max, y_max = min(img_w, x_max), min(img_h, y_max)\n",
                "        if x_max - x_min < 50 or y_max - y_min < 20: continue\n",
                "        lines_dict = defaultdict(list)\n",
                "        for w in sorted(words, key=lambda w: (w['line_num'], w['x'])):\n",
                "            lines_dict[w['line_num']].append(w['text'])\n",
                "        if len(lines_dict) < MIN_LINES: continue\n",
                "        tess_text = '\\n'.join(' '.join(ws) for ws in lines_dict.values())\n",
                "        matched_gt, score = find_best_sentence_range(tess_text, sentences)\n",
                "        if score < MIN_MATCH_SCORE or not matched_gt: continue\n",
                "        para_crop = img[y_min:y_max, x_min:x_max]\n",
                "        para_id = f\"{unique_id}_para_{para_idx:02d}\"\n",
                "        cv2.imwrite(str(output_dir / f\"{para_id}.jpg\"), para_crop)\n",
                "        results.append({\"id\": para_id, \"image\": f\"{para_id}.jpg\", \"text\": matched_gt, \"match_score\": score, \"line_count\": len(lines_dict)})\n",
                "        para_idx += 1\n",
                "    return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Run Pipeline\n",
                "print(\"ðŸš€ Telugu OCR Dataset Builder\")\n",
                "\n",
                "# Setup output\n",
                "if OUTPUT.exists(): shutil.rmtree(OUTPUT)\n",
                "OUTPUT.mkdir(parents=True)\n",
                "(OUTPUT / \"images\").mkdir()\n",
                "\n",
                "# Build index\n",
                "page_to_story = build_story_index()\n",
                "\n",
                "# Get all images\n",
                "all_images = list(SOURCE_IMAGES_DIR.rglob(\"*.jpg\"))\n",
                "print(f\"Total images: {len(all_images)}\")\n",
                "\n",
                "# Process\n",
                "all_results = []\n",
                "stats = {\"processed\": 0, \"with_gt\": 0, \"extracted\": 0}\n",
                "\n",
                "for img_path in tqdm(all_images):\n",
                "    stats[\"processed\"] += 1\n",
                "    pdf_folder = img_path.parent.name\n",
                "    pdf_stem = pdf_folder.replace(\" \", \"_\")\n",
                "    page_num = parse_page_number(img_path.name)\n",
                "    if page_num is None: continue\n",
                "    key = (pdf_stem, page_num)\n",
                "    gt_text = page_to_story.get(key)\n",
                "    if not gt_text: continue\n",
                "    stats[\"with_gt\"] += 1\n",
                "    year = img_path.parent.parent.name\n",
                "    unique_id = f\"{year}_{pdf_stem}_p{page_num:03d}\"\n",
                "    results = extract_and_align(img_path, gt_text, OUTPUT / \"images\", unique_id)\n",
                "    if results:\n",
                "        stats[\"extracted\"] += len(results)\n",
                "        all_results.extend(results)\n",
                "\n",
                "# Save\n",
                "with open(OUTPUT / \"metadata.jsonl\", 'w', encoding='utf-8') as f:\n",
                "    for item in all_results:\n",
                "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
                "\n",
                "print(f\"\\nâœ… Complete!\")\n",
                "print(f\"   Processed: {stats['processed']}\")\n",
                "print(f\"   With GT: {stats['with_gt']}\")\n",
                "print(f\"   High-confidence: {stats['extracted']}\")\n",
                "print(f\"   Output: {OUTPUT}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Check results\n",
                "!wc -l {OUTPUT}/metadata.jsonl\n",
                "!ls {OUTPUT}/images | head -20"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}