# Telugu OCR Dataset Preparation Pipeline

A specialized pipeline for creating a high-quality, large-scale Telugu OCR dataset from Chandamama magazine archives (1947â€“2012). This repository focuses on extraction, alignment, and Hugging Face upload.

## ğŸ¯ Objective
To generate a **Paragraph-Level Telugu OCR Dataset** suitable for fine-tuning modern Transformer models (TrOCR, PaddleOCR).

**Dataset Scale:** ~70,000 Paragraphs  
**Source:** Chandamama Kathalu (35k+ Pages)  
**Quality Control:** Strict fuzzy matching (>95%)

---

## ğŸ› ï¸ Pipeline Overview

1.  **Ingestion:** Reads raw scanned images and ground truth JSONs.
2.  **Processing (Local/Single System):**
    *   **OCR:** Tesseract (Telugu) extracts text from images.
    *   **Alignment:** Fuzzy matches OCR text with Ground Truth sentences.
    *   **Filtering:** STRICT checks (Min 95% match, Min 4 lines).
    *   **Cropping:** Extracts matching image regions.
3.  **Upload:** Pushes the final dataset to Hugging Face Hub.

---

## ğŸ“ Repository Structure

```
telugu-ocr-dataset/
â”œâ”€â”€ source_images/         # Raw scanned images (images/YYYY/...)
â”œâ”€â”€ ground_truth/          # Corresponding JSON files (YYYY/...)
â”œâ”€â”€ dataset/               # Output folder (Generated by script)
â”‚   â”œâ”€â”€ images/            # Cropped paragraph images
â”‚   â””â”€â”€ metadata.jsonl     # Hugging Face metadata
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_ocr_dataset.py  # MAIN Extraction Script
â”‚   â””â”€â”€ upload_to_hf.py       # Hugging Face Upload Script
â””â”€â”€ colab_archive/         # Archived parallel processing notebooks
```

---

## ğŸš€ How to Run

### 1. Setup
Install requirements:
```bash
pip install pytesseract rapidfuzz opencv-python tqdm huggingface_hub datasets
```
Ensure **Tesseract OCR** is installed and `tesseract_cmd` path is correct in `scripts/build_ocr_dataset.py`.

### 2. Run Extraction
Execute the robust single-system script. This utilizes standard multiprocessing to speed up extraction.
```bash
python scripts/build_ocr_dataset.py
```
*Creates `dataset/` folder with images and metadata.*

### 3. Upload to Hugging Face
Upload the processed dataset to your Hugging Face repository.
```bash
huggingface-cli upload <your-repo-id> dataset --repo-type dataset
```

---

## ğŸ“Š Dataset Metadata Format

The generated `metadata.jsonl` follows this schema:

```json
{
  "id": "1947_Chandamama_July_page_4_p01",
  "file_name": "1947_Chandamama_July_page_4_p01.jpg",
  "text": "Correct Ground truth text...",
  "ocr_text_tesseract": "Raw OCR output...",
  "match_score": 98,
  "line_count": 6,
  "origin_year": "1947",
  "origin_issue": "Chandamama_July"
}
```

---

## ğŸ”’ License
This code is open-source. The dataset contents are derived from public archives.
